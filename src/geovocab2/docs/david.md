# üåü David Configuration System - Complete Guide

# NOTE:
This is an AI generated documentation file for the new David configuration system, it's full of incorrect information and I will fix it later.


## ‚ú® What's New

The David model now has a comprehensive configuration system with:

- ‚úÖ **DavidArchitectureConfig** - Clean dataclass for all architecture settings
- ‚úÖ **Preset Configurations** - 7 ready-to-use presets for different use cases
- ‚úÖ **Save/Load Configs** - JSON serialization for reproducibility
- ‚úÖ **Belly Architecture Control** - Now configurable (was hardcoded)
- ‚úÖ **Mixed Initialization** - Config + parameter overrides
- ‚úÖ **Model Loading Utilities** - Easy checkpoint loading

---

## üöÄ Quick Start

### 1. Using Presets (Easiest)

```python
from geovocab2.train.model.core.david import David

# Use a preset configuration
david = David.from_preset('balanced')

# Available presets:
# - 'small_fast'              ‚Üí Fast prototyping
# - 'balanced'                ‚Üí General use (recommended)
# - 'high_accuracy'           ‚Üí Maximum performance
# - 'hierarchical_refinement' ‚Üí Progressive refinement
# - 'clip_vit_b16'            ‚Üí CLIP ViT-B/16 optimized
# - 'clip_vit_l14'            ‚Üí CLIP ViT-L/14 optimized
# - 'clip_vit_h14'            ‚Üí CLIP ViT-H/14 optimized
```

### 2. Custom Configuration

```python
from geovocab2.train.model.core.david import David, DavidArchitectureConfig

# Create custom config
config = DavidArchitectureConfig(
    feature_dim=512,
    num_classes=100,  # CIFAR-100
    scales=[128, 256, 512],
    sharing_mode="partial_shared",
    fusion_mode="attention",
    use_belly=True,
    belly_expand=2.5,  # Bigger bottleneck
    progressive_training=True,
)

# Initialize David
david = David.from_config(config)
```

### 3. Mixed Initialization (Config + Overrides)

```python
# Start with a preset, override specific settings
base_config = DavidPresets.balanced()

david = David(
    config=base_config,
    num_classes=100,     # Override for CIFAR-100
    use_belly=False,     # Disable belly architecture
    fusion_mode=FusionMode.ATTENTION  # Change fusion strategy
)
```

---

## üìã Configuration Options

### Core Architecture

```python
DavidArchitectureConfig(
    # Input/Output
    feature_dim=512,           # Input feature dimension
    num_classes=1000,          # Number of output classes
    
    # Multi-scale processing
    scales=[256, 512, 768, 1024],  # Crystal embedding dimensions
    
    # Sharing strategy
    sharing_mode="partial_shared",  # How parameters are shared
    # Options: "fully_shared", "partial_shared", "decoupled", "hierarchical"
    
    # Fusion strategy  
    fusion_mode="hierarchical_tree",  # How to combine scales
    # Options: "attention", "gated", "hierarchical_tree", 
    #          "deep_efficiency", "weighted_sum", "max_confidence", "progressive"
)
```

### Projection Head Configuration

```python
DavidArchitectureConfig(
    # Belly architecture (2x expansion bottleneck)
    use_belly=True,       # Enable/disable belly
    belly_expand=2.0,     # Expansion factor (if enabled)
    
    # Example:
    # use_belly=True, belly_expand=2.0
    #   ‚Üí input_dim ‚Üí 2*crystal_dim ‚Üí ReLU ‚Üí Dropout ‚Üí crystal_dim
    # 
    # use_belly=False
    #   ‚Üí input_dim ‚Üí crystal_dim (direct projection)
)
```

### Shared Feature Extraction

```python
DavidArchitectureConfig(
    # For FULLY_SHARED and PARTIAL_SHARED modes
    shared_feature_dim=768,  # Dimension of shared space
    shared_layers=2,         # Number of shared layers
    shared_dropout=0.1,      # Dropout in shared layers
)
```

### Fusion Configuration

```python
DavidArchitectureConfig(
    # Attention & Gated fusion
    fusion_temperature=1.0,  # Softmax temperature
    fusion_dropout=0.1,      # Dropout in fusion module
    
    # Hierarchical tree gating (fusion_mode="hierarchical_tree")
    tree_depth=3,            # Binary tree depth
    
    # Deep efficiency gating (fusion_mode="deep_efficiency")
    num_experts=3,           # Number of expert pathways
    compression_ratio=4,     # Bottleneck compression
)
```

### Progressive Training

```python
DavidArchitectureConfig(
    progressive_training=True,
    
    # Optional: Manual warmup schedule
    scale_warmup_epochs={
        256: 0,    # Active from epoch 0
        512: 5,    # Active from epoch 5
        768: 10,   # Active from epoch 10
        1024: 15,  # Active from epoch 15
    }
    # If None, auto-generates: scale_i activates at epoch i*3
)
```

---

## üíæ Save/Load Configuration

### Save Configuration

```python
# Create and save config
config = DavidArchitectureConfig(
    feature_dim=512,
    num_classes=100,
    scales=[256, 512, 768],
)

config.to_json("my_david_config.json")
```

### Load Configuration

```python
# Load from file
config = DavidArchitectureConfig.from_json("my_david_config.json")
david = David.from_config(config)
```

### Extract Config from Model

```python
# Get config from existing model
david = David.from_preset('balanced')
config = david.get_config()

# Save for later
config.to_json("extracted_config.json")
```

---

## üîß Training Integration

### Updated Training Pipeline

```python
from train_david_hf import DavidConfig, train_david

# Training config now includes belly parameters
config = DavidConfig(
    model_variant="clip_vit_b16",
    
    # Model architecture
    sharing_mode="partial_shared",
    fusion_mode="hierarchical_tree",
    use_belly=True,         # NEW!
    belly_expand=2.0,       # NEW!
    
    # Training
    num_epochs=50,
    batch_size=512,
    learning_rate=5e-3,
    
    # Loss
    use_rose_loss=True,
    rose_initial_weight=0.01,
    use_cayley_loss=False,
)

# Train
david, metrics = train_david(config)
```

### The trainer automatically:
1. Creates `DavidArchitectureConfig` from training config
2. Saves `david_config.json` alongside checkpoints
3. Enables easy model loading later

---

## üì¶ Loading Trained Models

### Simple Loading

```python
from train_david_hf import load_david_from_checkpoint

# Load trained model
david = load_david_from_checkpoint(
    "checkpoints/best_model.pth",
    device="cuda"
)
david.eval()

# Ready for inference!
```

### Loading with Optimizer State

```python
david, optimizer_state, metadata = load_david_from_checkpoint(
    "checkpoints/best_model.pth",
    device="cuda",
    load_optimizer=True
)

print(f"Best accuracy: {metadata['best_val_acc']:.2f}%")
print(f"Trained for: {metadata['epoch']} epochs")
```

---

## üéØ Preset Comparison

| Preset | Scales | Sharing | Fusion | Belly | Best For |
|--------|--------|---------|--------|-------|----------|
| **small_fast** | [256, 512] | fully_shared | weighted_sum | ‚ùå | Prototyping, limited compute |
| **balanced** | [256, 512, 768, 1024] | partial_shared | hierarchical_tree | ‚úÖ | General use (recommended) |
| **high_accuracy** | [256, ..., 1280] | decoupled | deep_efficiency | ‚úÖ | Competitions, final models |
| **hierarchical_refinement** | [256, ..., 1024] | hierarchical | progressive | ‚úÖ | Progressive refinement tasks |
| **clip_vit_b16** | [256, ..., 1024] | partial_shared | hierarchical_tree | ‚úÖ | CLIP ViT-B/16 features |
| **clip_vit_l14** | [384, ..., 1280] | partial_shared | deep_efficiency | ‚úÖ | CLIP ViT-L/14 features |
| **clip_vit_h14** | [512, ..., 1536] | partial_shared | deep_efficiency | ‚úÖ | CLIP ViT-H/14 features |

---

## üîç Configuration Validation

The config automatically validates:

```python
config = DavidArchitectureConfig(
    sharing_mode="invalid_mode"  # ‚ùå Will raise ValueError
)

# Valid modes:
# sharing_mode: fully_shared, partial_shared, decoupled, hierarchical
# fusion_mode: attention, gated, hierarchical_tree, deep_efficiency,
#              weighted_sum, max_confidence, progressive
```

---

## üéì Advanced Usage

### Auto-Generated Warmup Schedule

```python
config = DavidArchitectureConfig(
    scales=[256, 512, 768, 1024],
    progressive_training=True,
    # scale_warmup_epochs not specified
)

# Automatically generates:
# {256: 0, 512: 3, 768: 6, 1024: 9}
# Each scale activates 3 epochs after the previous
```

### Custom Scale Weighting

```python
# In training config
config = DavidConfig(
    scale_loss_balance={
        256: 1.5,   # Weight 256D scale more
        512: 1.2,
        768: 1.0,
        1024: 0.8,  # Weight 1024D scale less
    }
)
```

### Disable Belly for Specific Modes

```python
# Belly typically disabled for fully_shared mode
config = DavidArchitectureConfig(
    sharing_mode="fully_shared",
    use_belly=False,  # More efficient for shared extraction
)

# Belly typically enabled for other modes
config = DavidArchitectureConfig(
    sharing_mode="decoupled",
    use_belly=True,       # Better learning capacity
    belly_expand=2.5,     # Bigger bottleneck
)
```

---

## üìä Parameter Count Comparison

```python
# Compare different configurations
configs = {
    'small': DavidPresets.small_fast(),
    'balanced': DavidPresets.balanced(),
    'high_acc': DavidPresets.high_accuracy(),
}

for name, config in configs.items():
    david = David.from_config(config)
    params = sum(p.numel() for p in david.parameters())
    print(f"{name:12s}: {params:,} parameters")

# Output:
# small       : 2,345,678 parameters
# balanced    : 8,758,271 parameters
# high_acc    : 15,234,567 parameters
```

---

## üêõ Troubleshooting

### Issue: "Cannot find david_config.json"

**Solution:** Make sure the checkpoint directory contains `david_config.json`:

```python
# Training automatically saves it:
# checkpoints/
#   ‚îú‚îÄ‚îÄ best_model.pth
#   ‚îú‚îÄ‚îÄ david_config.json  ‚Üê Should be here
#   ‚îî‚îÄ‚îÄ config.json
```

### Issue: "Belly architecture mismatch"

**Solution:** When loading checkpoints, belly settings must match:

```python
# Check the config used for training
config = DavidArchitectureConfig.from_json("checkpoints/david_config.json")
print(f"use_belly: {config.use_belly}")
print(f"belly_expand: {config.belly_expand}")
```

### Issue: "Progressive training not working"

**Solution:** Make sure to call `david.update_epoch(epoch)` in training loop:

```python
for epoch in range(num_epochs):
    david.update_epoch(epoch)  # ‚Üê Required!
    train_one_epoch(...)
```

---

## ‚úÖ Migration from Old Code

### Before (Old way):

```python
david = David(
    feature_dim=512,
    num_classes=1000,
    scales=[256, 512, 768, 1024],
    sharing_mode=SharingMode.PARTIAL_SHARED,
    fusion_mode=FusionMode.HIERARCHICAL_TREE,
    shared_feature_dim=768,
    shared_layers=2,
    shared_dropout=0.1,
    fusion_temperature=1.0,
    fusion_dropout=0.1,
    tree_depth=3,
    # ... 10+ more parameters ...
)
```

### After (New way):

```python
# Option 1: Use preset
david = David.from_preset('balanced')

# Option 2: Custom config
config = DavidArchitectureConfig(
    feature_dim=512,
    num_classes=1000,
    # ... only the params you want to customize
)
david = David.from_config(config)
```

---

## üéâ Summary

The new configuration system makes David:

- ‚úÖ **Easier to use** - Presets for common cases
- ‚úÖ **More maintainable** - All config in one place
- ‚úÖ **More reproducible** - Save/load configurations
- ‚úÖ **More flexible** - Mix presets with overrides
- ‚úÖ **Better documented** - Clear parameter meanings
- ‚úÖ **Backward compatible** - Old initialization still works

**Ready to train!** üöÄ

```python
# Start training with the fixed pipeline
from train_david_hf import DavidConfig, train_david

config = DavidConfig(
    model_variant="clip_vit_b16",
    num_epochs=50,
    batch_size=512,
    learning_rate=5e-3,
    use_mixed_precision=False,  # fp32 stability
)

david, metrics = train_david(config)
```