{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fdg-e7t0qZu",
        "outputId": "3f0c0d05-749e-460a-f2ef-855ebb2cd679"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping geometricvocab as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping geofractal as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for geofractal (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for geometricvocab (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    !pip uninstall -qy geometricvocab geofractal\n",
        "except:\n",
        "    pass\n",
        "\n",
        "!pip install -q git+https://github.com/AbstractEyes/geofractal.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ineffective CantorGELU on MNIST"
      ],
      "metadata": {
        "id": "-fO6x7lV-MN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import time\n",
        "\n",
        "# =============================================================================\n",
        "# FIXED CANTOR GELU KERNELS (numerically stable)\n",
        "# =============================================================================\n",
        "\n",
        "@triton.jit\n",
        "def cantor_gelu_fwd_kernel(x_ptr, out_ptr, n_elements, step, strength, BLOCK: tl.constexpr):\n",
        "    pid = tl.program_id(0)\n",
        "    offs = pid * BLOCK + tl.arange(0, BLOCK)\n",
        "    mask = offs < n_elements\n",
        "\n",
        "    x = tl.load(x_ptr + offs, mask=mask)\n",
        "\n",
        "    # GELU with numerically stable tanh\n",
        "    x3 = x * x * x\n",
        "    inner = 0.7978845608028654 * (x + 0.044715 * x3)\n",
        "\n",
        "    # Stable tanh: clamp input to prevent exp overflow\n",
        "    inner_clamped = tl.minimum(tl.maximum(inner, -10.0), 10.0)\n",
        "    e2 = tl.exp(2.0 * inner_clamped)\n",
        "    tanh_val = (e2 - 1.0) / (e2 + 1.0)\n",
        "\n",
        "    gelu = 0.5 * x * (1.0 + tanh_val)\n",
        "\n",
        "    # Staircase (also clamp x to prevent issues)\n",
        "    x_clamped = tl.minimum(tl.maximum(x, -10.0), 10.0)\n",
        "    snapped = tl.floor(x_clamped / step) * step\n",
        "\n",
        "    out = strength * snapped + (1.0 - strength) * gelu\n",
        "    tl.store(out_ptr + offs, out, mask=mask)\n",
        "\n",
        "\n",
        "@triton.jit\n",
        "def cantor_gelu_bwd_kernel(grad_out_ptr, x_ptr, grad_x_ptr, n_elements, step, strength, BLOCK: tl.constexpr):\n",
        "    pid = tl.program_id(0)\n",
        "    offs = pid * BLOCK + tl.arange(0, BLOCK)\n",
        "    mask = offs < n_elements\n",
        "\n",
        "    grad_out = tl.load(grad_out_ptr + offs, mask=mask)\n",
        "    x = tl.load(x_ptr + offs, mask=mask)\n",
        "\n",
        "    x3 = x * x * x\n",
        "    inner = 0.7978845608028654 * (x + 0.044715 * x3)\n",
        "\n",
        "    # Stable tanh\n",
        "    inner_clamped = tl.minimum(tl.maximum(inner, -10.0), 10.0)\n",
        "    e2 = tl.exp(2.0 * inner_clamped)\n",
        "    tanh_inner = (e2 - 1.0) / (e2 + 1.0)\n",
        "\n",
        "    sech2 = 1.0 - tanh_inner * tanh_inner\n",
        "    f_prime = 0.7978845608028654 * (1.0 + 0.134145 * x * x)\n",
        "    gelu_grad = 0.5 * (1.0 + tanh_inner) + 0.5 * x * sech2 * f_prime\n",
        "\n",
        "    # Clamp gradient for stability\n",
        "    gelu_grad = tl.minimum(tl.maximum(gelu_grad, -10.0), 10.0)\n",
        "\n",
        "    grad_x = grad_out * (strength + (1.0 - strength) * gelu_grad)\n",
        "    tl.store(grad_x_ptr + offs, grad_x, mask=mask)\n",
        "\n",
        "\n",
        "class _CantorGELUFunc(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, step, strength):\n",
        "        out = torch.empty_like(x)\n",
        "        n = x.numel()\n",
        "        cantor_gelu_fwd_kernel[(triton.cdiv(n, 1024),)](x, out, n, step, strength, BLOCK=1024)\n",
        "        ctx.save_for_backward(x)\n",
        "        ctx.step, ctx.strength = step, strength\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_out):\n",
        "        x, = ctx.saved_tensors\n",
        "        grad_x = torch.empty_like(x)\n",
        "        n = x.numel()\n",
        "        cantor_gelu_bwd_kernel[(triton.cdiv(n, 1024),)](\n",
        "            grad_out.contiguous(), x, grad_x, n, ctx.step, ctx.strength, BLOCK=1024\n",
        "        )\n",
        "        return grad_x, None, None\n",
        "\n",
        "\n",
        "class CantorGELU(nn.Module):\n",
        "    def __init__(self, num_stairs: int = 16, value_range: float = 8.0, init_strength: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.step = 2 * value_range / num_stairs\n",
        "        self.strength = nn.Parameter(torch.tensor(init_strength))\n",
        "        self.num_stairs = num_stairs\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if not self.training:\n",
        "            return F.gelu(x)\n",
        "        s = torch.sigmoid(self.strength).item()\n",
        "        return _CantorGELUFunc.apply(x.contiguous(), self.step, s)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIG\n",
        "# =============================================================================\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "LR = 1e-3\n",
        "NUM_STAIRS = 8\n",
        "\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"CantorGELU stairs: {NUM_STAIRS}\")\n",
        "\n",
        "# =============================================================================\n",
        "# DATA\n",
        "# =============================================================================\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train: {len(train_dataset):,} samples\")\n",
        "print(f\"Test:  {len(test_dataset):,} samples\")\n",
        "\n",
        "# =============================================================================\n",
        "# MODEL\n",
        "# =============================================================================\n",
        "\n",
        "class MNISTClassifier(nn.Module):\n",
        "    def __init__(self, activation='cantor', num_stairs=16):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "\n",
        "        self.fc1 = nn.Linear(128 * 3 * 3, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
        "        self.bn_fc2 = nn.BatchNorm1d(128)\n",
        "\n",
        "        if activation == 'cantor':\n",
        "            self.act = CantorGELU(num_stairs=num_stairs)\n",
        "            self.act_name = f'CantorGELU({num_stairs})'\n",
        "        elif activation == 'gelu':\n",
        "            self.act = nn.GELU()\n",
        "            self.act_name = 'GELU'\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown activation: {activation}\")\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.act(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(self.act(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(self.act(self.bn3(self.conv3(x))))\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = self.dropout(self.act(self.bn_fc1(self.fc1(x))))\n",
        "        x = self.dropout(self.act(self.bn_fc2(self.fc2(x))))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(model, loader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for data, target in loader:\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping for stability\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += pred.eq(target).sum().item()\n",
        "        total += target.size(0)\n",
        "\n",
        "    return total_loss / len(loader), 100. * correct / total\n",
        "\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            output = model(data)\n",
        "            total_loss += F.cross_entropy(output, target).item()\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "    return total_loss / len(loader), 100. * correct / total\n",
        "\n",
        "\n",
        "def train_model(activation='cantor', num_stairs=16):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training with {activation.upper()}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model = MNISTClassifier(activation=activation, num_stairs=num_stairs).to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Parameters: {num_params:,}\")\n",
        "\n",
        "    best_acc = 0\n",
        "    history = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer)\n",
        "        test_loss, test_acc = evaluate(model, test_loader)\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        best_marker = \" *\" if test_acc > best_acc else \"\"\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "\n",
        "        print(f\"E{epoch:02d} | Train: {train_loss:.4f} / {train_acc:.2f}% | \"\n",
        "              f\"Test: {test_loss:.4f} / {test_acc:.2f}%{best_marker} | {epoch_time:.1f}s\")\n",
        "\n",
        "        history.append({'epoch': epoch, 'train_loss': train_loss, 'train_acc': train_acc,\n",
        "                       'test_loss': test_loss, 'test_acc': test_acc, 'time': epoch_time})\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"Best: {best_acc:.2f}% | Total: {total_time:.1f}s\")\n",
        "\n",
        "    return model, history, best_acc\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# RUN\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MNIST CLASSIFIER - CANTOR GELU vs GELU\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "cantor_model, cantor_history, cantor_best = train_model('cantor', num_stairs=NUM_STAIRS)\n",
        "gelu_model, gelu_history, gelu_best = train_model('gelu')\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "cantor_time = sum(h['time'] for h in cantor_history)\n",
        "#gelu_time = sum(h['time'] for h in gelu_history)\n",
        "\n",
        "print(f\"\\n{'Activation':<20} {'Best Acc':<12} {'Time':<12}\")\n",
        "print(\"-\"*44)\n",
        "print(f\"{'CantorGELU':<20} {cantor_best:.2f}%{'':<6} {cantor_time:.1f}s\")\n",
        "#print(f\"{'GELU':<20} {gelu_best:.2f}%{'':<6} {gelu_time:.1f}s\")\n",
        "#print(f\"\\nDiff: {cantor_best - gelu_best:+.2f}% | Overhead: {cantor_time/gelu_time:.2f}x\")\n",
        "print(f\"Learned strength: {torch.sigmoid(cantor_model.act.strength).item():.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYmzZa-u00N_",
        "outputId": "ea6398b5-7d34-4232-e05b-59106f4041ef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "CantorGELU stairs: 8\n",
            "Train: 60,000 samples\n",
            "Test:  10,000 samples\n",
            "\n",
            "============================================================\n",
            "MNIST CLASSIFIER - CANTOR GELU vs GELU\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Training with CANTOR\n",
            "============================================================\n",
            "Parameters: 423,243\n",
            "E01 | Train: 0.1155 / 96.93% | Test: 0.4026 / 95.63% * | 8.3s\n",
            "E02 | Train: 0.0466 / 98.62% | Test: 0.2316 / 97.83% * | 8.2s\n",
            "E03 | Train: 0.0324 / 98.98% | Test: 0.2443 / 95.51% | 8.0s\n",
            "E04 | Train: 0.0253 / 99.22% | Test: 0.1452 / 98.11% * | 8.5s\n",
            "E05 | Train: 0.0173 / 99.44% | Test: 0.1666 / 96.55% | 8.5s\n",
            "E06 | Train: 0.0140 / 99.54% | Test: 0.1389 / 97.29% | 8.6s\n",
            "E07 | Train: 0.0090 / 99.72% | Test: 0.1081 / 98.00% | 8.1s\n",
            "E08 | Train: 0.0059 / 99.84% | Test: 0.1277 / 97.44% | 8.5s\n",
            "E09 | Train: 0.0039 / 99.91% | Test: 0.0920 / 98.34% * | 8.5s\n",
            "E10 | Train: 0.0032 / 99.94% | Test: 0.1060 / 98.03% | 8.2s\n",
            "Best: 98.34% | Total: 83.2s\n",
            "\n",
            "============================================================\n",
            "SUMMARY\n",
            "============================================================\n",
            "\n",
            "Activation           Best Acc     Time        \n",
            "--------------------------------------------\n",
            "CantorGELU           98.34%       83.2s\n",
            "Learned strength: 0.525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# dropouts MNIST"
      ],
      "metadata": {
        "id": "usKNcNVz-JUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "\n",
        "# =============================================================================\n",
        "# TOPOLOGICAL DROPOUT VARIANTS\n",
        "# =============================================================================\n",
        "\n",
        "class TopologicalDropout(nn.Module):\n",
        "    \"\"\"\n",
        "    Structure-preserving dropout: drops entire routes/channels, not individual neurons.\n",
        "\n",
        "    For CNNs: treats channels as routes (drops entire feature maps)\n",
        "    For attention: treats heads/routes as units\n",
        "\n",
        "    Key insight: Preserves internal structure of surviving routes.\n",
        "    Standard dropout: Random holes everywhere → broken features\n",
        "    Topo dropout: Some features fully on, others fully off → intact features\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob: float = 0.1, min_keep: int = 1, route_dim: int = 1):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.min_keep = min_keep\n",
        "        self.route_dim = route_dim  # Which dim contains \"routes\" (channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if not self.training or self.drop_prob == 0:\n",
        "            return x\n",
        "\n",
        "        num_routes = x.shape[self.route_dim]\n",
        "        num_keep = max(self.min_keep, int(num_routes * (1 - self.drop_prob)))\n",
        "\n",
        "        # Random mask\n",
        "        mask = torch.zeros(num_routes, device=x.device)\n",
        "        perm = torch.randperm(num_routes, device=x.device)[:num_keep]\n",
        "        mask[perm] = 1.0\n",
        "\n",
        "        # Scale to preserve expected value\n",
        "        mask = mask * (num_routes / num_keep)\n",
        "\n",
        "        # Reshape for broadcast\n",
        "        shape = [1] * x.dim()\n",
        "        shape[self.route_dim] = num_routes\n",
        "\n",
        "        return x * mask.view(shape)\n",
        "\n",
        "\n",
        "class ImportanceTopologicalDropout(nn.Module):\n",
        "    \"\"\"\n",
        "    Topo dropout with importance weighting.\n",
        "    Less important routes more likely to be dropped.\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob: float = 0.1, min_keep: int = 1, route_dim: int = 1):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.min_keep = min_keep\n",
        "        self.route_dim = route_dim\n",
        "\n",
        "    def forward(self, x: torch.Tensor, importance: torch.Tensor = None) -> torch.Tensor:\n",
        "        if not self.training or self.drop_prob == 0:\n",
        "            return x\n",
        "\n",
        "        num_routes = x.shape[self.route_dim]\n",
        "        num_keep = max(self.min_keep, int(num_routes * (1 - self.drop_prob)))\n",
        "\n",
        "        if importance is None:\n",
        "            # Use activation magnitude as importance proxy\n",
        "            # Reduce all dims except route_dim\n",
        "            reduce_dims = [i for i in range(x.dim()) if i != self.route_dim]\n",
        "            importance = x.abs().mean(dim=reduce_dims)\n",
        "\n",
        "        # Add noise to importance for stochasticity\n",
        "        noise = torch.rand_like(importance) * 0.3\n",
        "        scores = importance + noise\n",
        "\n",
        "        # Keep top-k by importance\n",
        "        _, keep_idx = scores.topk(num_keep)\n",
        "        mask = torch.zeros(num_routes, device=x.device)\n",
        "        mask[keep_idx] = 1.0\n",
        "\n",
        "        # Scale\n",
        "        mask = mask * (num_routes / num_keep)\n",
        "\n",
        "        shape = [1] * x.dim()\n",
        "        shape[self.route_dim] = num_routes\n",
        "\n",
        "        return x * mask.view(shape)\n",
        "\n",
        "\n",
        "class ScheduledTopologicalDropout(nn.Module):\n",
        "    \"\"\"\n",
        "    Topo dropout with warmup schedule.\n",
        "    Starts mild, increases over training.\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob: float = 0.2, min_keep: int = 1, route_dim: int = 1,\n",
        "                 warmup_steps: int = 1000):\n",
        "        super().__init__()\n",
        "        self.target_drop_prob = drop_prob\n",
        "        self.min_keep = min_keep\n",
        "        self.route_dim = route_dim\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.register_buffer('step', torch.tensor(0))\n",
        "\n",
        "    @property\n",
        "    def current_drop_prob(self):\n",
        "        progress = min(1.0, self.step.item() / self.warmup_steps)\n",
        "        return self.target_drop_prob * progress\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.training:\n",
        "            self.step += 1\n",
        "\n",
        "        if not self.training or self.current_drop_prob == 0:\n",
        "            return x\n",
        "\n",
        "        num_routes = x.shape[self.route_dim]\n",
        "        num_keep = max(self.min_keep, int(num_routes * (1 - self.current_drop_prob)))\n",
        "\n",
        "        mask = torch.zeros(num_routes, device=x.device)\n",
        "        perm = torch.randperm(num_routes, device=x.device)[:num_keep]\n",
        "        mask[perm] = 1.0\n",
        "        mask = mask * (num_routes / num_keep)\n",
        "\n",
        "        shape = [1] * x.dim()\n",
        "        shape[self.route_dim] = num_routes\n",
        "\n",
        "        return x * mask.view(shape)\n",
        "\n",
        "\n",
        "class SpatialTopologicalDropout(nn.Module):\n",
        "    \"\"\"\n",
        "    For 2D feature maps: drops entire spatial regions (patches).\n",
        "    Complements channel-wise topo dropout.\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob: float = 0.1, patch_size: int = 2):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if not self.training or self.drop_prob == 0:\n",
        "            return x\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "        pH, pW = H // self.patch_size, W // self.patch_size\n",
        "\n",
        "        if pH == 0 or pW == 0:\n",
        "            return x\n",
        "\n",
        "        # Create patch mask\n",
        "        mask = (torch.rand(B, 1, pH, pW, device=x.device) > self.drop_prob).float()\n",
        "\n",
        "        # Scale surviving patches\n",
        "        keep_ratio = mask.mean()\n",
        "        if keep_ratio > 0:\n",
        "            mask = mask / keep_ratio\n",
        "\n",
        "        # Upsample mask to full resolution\n",
        "        mask = F.interpolate(mask, size=(H, W), mode='nearest')\n",
        "\n",
        "        return x * mask\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIG\n",
        "# =============================================================================\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "LR = 1e-3\n",
        "\n",
        "print(f\"Device: {DEVICE}\")\n",
        "\n",
        "# =============================================================================\n",
        "# DATA\n",
        "# =============================================================================\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train: {len(train_dataset):,} | Test: {len(test_dataset):,}\")\n",
        "\n",
        "# =============================================================================\n",
        "# MODEL\n",
        "# =============================================================================\n",
        "\n",
        "class MNISTClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN with configurable dropout strategy.\n",
        "\n",
        "    dropout_type options:\n",
        "    - 'standard': nn.Dropout (element-wise)\n",
        "    - 'topo': TopologicalDropout (channel-wise)\n",
        "    - 'topo_importance': ImportanceTopologicalDropout\n",
        "    - 'topo_scheduled': ScheduledTopologicalDropout\n",
        "    - 'spatial': SpatialTopologicalDropout\n",
        "    - 'topo_spatial': Both channel and spatial topo dropout\n",
        "    - 'none': No dropout\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout_type='topo', drop_prob=0.2):\n",
        "        super().__init__()\n",
        "        self.dropout_type = dropout_type\n",
        "\n",
        "        # Conv layers\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "\n",
        "        # FC layers\n",
        "        self.fc1 = nn.Linear(128 * 3 * 3, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "        # Batch norms\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
        "        self.bn_fc2 = nn.BatchNorm1d(128)\n",
        "\n",
        "        # Dropout variants\n",
        "        if dropout_type == 'standard':\n",
        "            self.drop_conv = nn.Dropout2d(drop_prob)\n",
        "            self.drop_fc = nn.Dropout(drop_prob)\n",
        "            self.spatial_drop = None\n",
        "        elif dropout_type == 'topo':\n",
        "            self.drop_conv = TopologicalDropout(drop_prob, min_keep=4, route_dim=1)\n",
        "            self.drop_fc = TopologicalDropout(drop_prob, min_keep=16, route_dim=1)\n",
        "            self.spatial_drop = None\n",
        "        elif dropout_type == 'topo_importance':\n",
        "            self.drop_conv = ImportanceTopologicalDropout(drop_prob, min_keep=4, route_dim=1)\n",
        "            self.drop_fc = ImportanceTopologicalDropout(drop_prob, min_keep=16, route_dim=1)\n",
        "            self.spatial_drop = None\n",
        "        elif dropout_type == 'topo_scheduled':\n",
        "            self.drop_conv = ScheduledTopologicalDropout(drop_prob, min_keep=4, route_dim=1, warmup_steps=500)\n",
        "            self.drop_fc = ScheduledTopologicalDropout(drop_prob, min_keep=16, route_dim=1, warmup_steps=500)\n",
        "            self.spatial_drop = None\n",
        "        elif dropout_type == 'spatial':\n",
        "            self.drop_conv = SpatialTopologicalDropout(drop_prob, patch_size=2)\n",
        "            self.drop_fc = nn.Dropout(drop_prob)\n",
        "            self.spatial_drop = None\n",
        "        elif dropout_type == 'topo_spatial':\n",
        "            self.drop_conv = TopologicalDropout(drop_prob * 0.5, min_keep=4, route_dim=1)\n",
        "            self.drop_fc = TopologicalDropout(drop_prob, min_keep=16, route_dim=1)\n",
        "            self.spatial_drop = SpatialTopologicalDropout(drop_prob * 0.5, patch_size=2)\n",
        "        elif dropout_type == 'none':\n",
        "            self.drop_conv = nn.Identity()\n",
        "            self.drop_fc = nn.Identity()\n",
        "            self.spatial_drop = None\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown dropout_type: {dropout_type}\")\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Conv block 1: 28x28 -> 14x14\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop_conv(x)\n",
        "        if self.spatial_drop:\n",
        "            x = self.spatial_drop(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Conv block 2: 14x14 -> 7x7\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop_conv(x)\n",
        "        if self.spatial_drop:\n",
        "            x = self.spatial_drop(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Conv block 3: 7x7 -> 3x3\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop_conv(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # FC layers\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn_fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop_fc(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn_fc2(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop_fc(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(model, loader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for data, target in loader:\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += pred.eq(target).sum().item()\n",
        "        total += target.size(0)\n",
        "\n",
        "    return total_loss / len(loader), 100. * correct / total\n",
        "\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            output = model(data)\n",
        "            total_loss += F.cross_entropy(output, target).item()\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "    return total_loss / len(loader), 100. * correct / total\n",
        "\n",
        "\n",
        "def train_model(dropout_type='topo', drop_prob=0.2, verbose=True):\n",
        "    model = MNISTClassifier(dropout_type=dropout_type, drop_prob=drop_prob).to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Training: {dropout_type} (p={drop_prob})\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "    best_acc = 0\n",
        "    history = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer)\n",
        "        test_loss, test_acc = evaluate(model, test_loader)\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        best_marker = \" *\" if test_acc > best_acc else \"\"\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"E{epoch:02d} | Train: {train_loss:.4f} / {train_acc:.2f}% | \"\n",
        "                  f\"Test: {test_loss:.4f} / {test_acc:.2f}%{best_marker} | {epoch_time:.1f}s\")\n",
        "\n",
        "        history.append({'epoch': epoch, 'train_loss': train_loss, 'train_acc': train_acc,\n",
        "                       'test_loss': test_loss, 'test_acc': test_acc, 'time': epoch_time})\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Best: {best_acc:.2f}% | Total: {total_time:.1f}s\")\n",
        "\n",
        "    return model, history, best_acc, total_time\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# RUN EXPERIMENTS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TOPOLOGICAL DROPOUT EXPERIMENT - MNIST\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Test each dropout variant\n",
        "dropout_configs = [\n",
        "    ('none', 0.0),\n",
        "    ('standard', 0.2),\n",
        "    ('topo', 0.2),\n",
        "    ('topo_importance', 0.2),\n",
        "    ('topo_scheduled', 0.2),\n",
        "    ('spatial', 0.2),\n",
        "    ('topo_spatial', 0.2),\n",
        "]\n",
        "\n",
        "for dropout_type, drop_prob in dropout_configs:\n",
        "    model, history, best_acc, total_time = train_model(dropout_type, drop_prob)\n",
        "    results[dropout_type] = {\n",
        "        'best_acc': best_acc,\n",
        "        'final_acc': history[-1]['test_acc'],\n",
        "        'time': total_time,\n",
        "        'history': history\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n{'Dropout Type':<20} {'Best Acc':<12} {'Final Acc':<12} {'Time':<10} {'Train-Test Gap':<15}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Sort by best accuracy\n",
        "sorted_results = sorted(results.items(), key=lambda x: x[1]['best_acc'], reverse=True)\n",
        "\n",
        "for dropout_type, data in sorted_results:\n",
        "    final_train = data['history'][-1]['train_acc']\n",
        "    final_test = data['history'][-1]['test_acc']\n",
        "    gap = final_train - final_test\n",
        "    print(f\"{dropout_type:<20} {data['best_acc']:<12.2f}% {data['final_acc']:<12.2f}% \"\n",
        "          f\"{data['time']:<10.1f}s {gap:<15.2f}%\")\n",
        "\n",
        "# Best vs baseline\n",
        "baseline = results['standard']['best_acc']\n",
        "print(f\"\\n--- vs Standard Dropout ---\")\n",
        "for dropout_type, data in sorted_results:\n",
        "    if dropout_type != 'standard':\n",
        "        diff = data['best_acc'] - baseline\n",
        "        print(f\"{dropout_type:<20} {diff:+.2f}%\")\n",
        "\n",
        "# =============================================================================\n",
        "# GENERALIZATION ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"GENERALIZATION ANALYSIS (Train-Test Gap Over Time)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n{'Dropout Type':<20} {'E1 Gap':<10} {'E5 Gap':<10} {'E10 Gap':<10} {'Trend':<10}\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "for dropout_type, data in sorted_results:\n",
        "    h = data['history']\n",
        "    gap_1 = h[0]['train_acc'] - h[0]['test_acc']\n",
        "    gap_5 = h[4]['train_acc'] - h[4]['test_acc']\n",
        "    gap_10 = h[9]['train_acc'] - h[9]['test_acc']\n",
        "\n",
        "    if gap_10 < gap_1:\n",
        "        trend = \"↓ good\"\n",
        "    elif gap_10 > gap_1 + 1:\n",
        "        trend = \"↑ overfit\"\n",
        "    else:\n",
        "        trend = \"→ stable\"\n",
        "\n",
        "    print(f\"{dropout_type:<20} {gap_1:<10.2f}% {gap_5:<10.2f}% {gap_10:<10.2f}% {trend:<10}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs_WOM0m7Gc-",
        "outputId": "940c1a08-2fa3-4384-fc70-89f321a5f364"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Train: 60,000 | Test: 10,000\n",
            "\n",
            "======================================================================\n",
            "TOPOLOGICAL DROPOUT EXPERIMENT - MNIST\n",
            "======================================================================\n",
            "\n",
            "============================================================\n",
            "Training: none (p=0.0)\n",
            "============================================================\n",
            "E01 | Train: 0.1001 / 97.71% | Test: 0.0351 / 98.95% * | 8.2s\n",
            "E02 | Train: 0.0311 / 99.08% | Test: 0.0281 / 99.14% * | 8.4s\n",
            "E03 | Train: 0.0191 / 99.41% | Test: 0.0295 / 99.01% | 8.6s\n",
            "E04 | Train: 0.0112 / 99.67% | Test: 0.0244 / 99.22% * | 8.3s\n",
            "E05 | Train: 0.0071 / 99.79% | Test: 0.0196 / 99.31% * | 8.4s\n",
            "E06 | Train: 0.0043 / 99.88% | Test: 0.0252 / 99.23% | 8.2s\n",
            "E07 | Train: 0.0021 / 99.95% | Test: 0.0183 / 99.40% * | 8.8s\n",
            "E08 | Train: 0.0009 / 99.99% | Test: 0.0183 / 99.37% | 8.0s\n",
            "E09 | Train: 0.0005 / 100.00% | Test: 0.0182 / 99.36% | 8.1s\n",
            "E10 | Train: 0.0004 / 100.00% | Test: 0.0176 / 99.39% | 8.3s\n",
            "Best: 99.40% | Total: 83.3s\n",
            "\n",
            "============================================================\n",
            "Training: standard (p=0.2)\n",
            "============================================================\n",
            "E01 | Train: 0.1947 / 95.04% | Test: 0.0356 / 98.79% * | 8.1s\n",
            "E02 | Train: 0.0562 / 98.29% | Test: 0.0245 / 99.12% * | 8.2s\n",
            "E03 | Train: 0.0408 / 98.79% | Test: 0.0206 / 99.34% * | 8.1s\n",
            "E04 | Train: 0.0328 / 98.99% | Test: 0.0261 / 99.23% | 8.0s\n",
            "E05 | Train: 0.0275 / 99.15% | Test: 0.0184 / 99.45% * | 8.0s\n",
            "E06 | Train: 0.0211 / 99.33% | Test: 0.0164 / 99.51% * | 8.3s\n",
            "E07 | Train: 0.0167 / 99.50% | Test: 0.0155 / 99.50% | 8.3s\n",
            "E08 | Train: 0.0135 / 99.58% | Test: 0.0146 / 99.52% * | 8.0s\n",
            "E09 | Train: 0.0111 / 99.68% | Test: 0.0154 / 99.48% | 8.2s\n",
            "E10 | Train: 0.0101 / 99.69% | Test: 0.0151 / 99.49% | 8.3s\n",
            "Best: 99.52% | Total: 81.6s\n",
            "\n",
            "============================================================\n",
            "Training: topo (p=0.2)\n",
            "============================================================\n",
            "E01 | Train: 0.1665 / 96.08% | Test: 0.0353 / 98.86% * | 8.1s\n",
            "E02 | Train: 0.0492 / 98.56% | Test: 0.0274 / 99.07% * | 8.1s\n",
            "E03 | Train: 0.0365 / 98.91% | Test: 0.0285 / 99.17% * | 8.3s\n",
            "E04 | Train: 0.0290 / 99.09% | Test: 0.0207 / 99.26% * | 8.2s\n",
            "E05 | Train: 0.0237 / 99.27% | Test: 0.0202 / 99.30% * | 8.1s\n",
            "E06 | Train: 0.0178 / 99.46% | Test: 0.0179 / 99.35% * | 8.0s\n",
            "E07 | Train: 0.0137 / 99.57% | Test: 0.0166 / 99.36% * | 8.5s\n",
            "E08 | Train: 0.0109 / 99.65% | Test: 0.0182 / 99.34% | 8.0s\n",
            "E09 | Train: 0.0095 / 99.70% | Test: 0.0165 / 99.39% * | 8.1s\n",
            "E10 | Train: 0.0079 / 99.77% | Test: 0.0160 / 99.42% * | 8.7s\n",
            "Best: 99.42% | Total: 82.0s\n",
            "\n",
            "============================================================\n",
            "Training: topo_importance (p=0.2)\n",
            "============================================================\n",
            "E01 | Train: 0.1482 / 96.44% | Test: 0.1155 / 96.32% * | 8.1s\n",
            "E02 | Train: 0.0423 / 98.73% | Test: 0.0757 / 97.44% * | 8.3s\n",
            "E03 | Train: 0.0318 / 99.02% | Test: 0.0303 / 99.02% * | 8.3s\n",
            "E04 | Train: 0.0235 / 99.26% | Test: 0.0307 / 98.98% | 8.2s\n",
            "E05 | Train: 0.0185 / 99.43% | Test: 0.0354 / 98.94% | 8.2s\n",
            "E06 | Train: 0.0131 / 99.60% | Test: 0.0254 / 99.17% * | 8.5s\n",
            "E07 | Train: 0.0103 / 99.69% | Test: 0.0258 / 99.16% | 8.4s\n",
            "E08 | Train: 0.0076 / 99.77% | Test: 0.0227 / 99.31% * | 8.5s\n",
            "E09 | Train: 0.0056 / 99.85% | Test: 0.0236 / 99.23% | 8.4s\n",
            "E10 | Train: 0.0049 / 99.88% | Test: 0.0209 / 99.32% * | 8.2s\n",
            "Best: 99.32% | Total: 83.1s\n",
            "\n",
            "============================================================\n",
            "Training: topo_scheduled (p=0.2)\n",
            "============================================================\n",
            "E01 | Train: 0.1168 / 97.28% | Test: 0.0323 / 98.96% * | 8.4s\n",
            "E02 | Train: 0.0463 / 98.57% | Test: 0.0275 / 99.08% * | 8.1s\n",
            "E03 | Train: 0.0321 / 99.04% | Test: 0.0243 / 99.13% * | 8.6s\n",
            "E04 | Train: 0.0269 / 99.15% | Test: 0.0210 / 99.30% * | 8.2s\n",
            "E05 | Train: 0.0218 / 99.28% | Test: 0.0201 / 99.44% * | 8.2s\n",
            "E06 | Train: 0.0160 / 99.50% | Test: 0.0196 / 99.36% | 8.3s\n",
            "E07 | Train: 0.0128 / 99.62% | Test: 0.0164 / 99.44% | 8.5s\n",
            "E08 | Train: 0.0108 / 99.69% | Test: 0.0170 / 99.42% | 8.2s\n",
            "E09 | Train: 0.0087 / 99.72% | Test: 0.0152 / 99.52% * | 8.3s\n",
            "E10 | Train: 0.0076 / 99.78% | Test: 0.0151 / 99.55% * | 8.5s\n",
            "Best: 99.55% | Total: 83.3s\n",
            "\n",
            "============================================================\n",
            "Training: spatial (p=0.2)\n",
            "============================================================\n",
            "E01 | Train: 0.2099 / 94.48% | Test: 0.0322 / 98.90% * | 8.6s\n",
            "E02 | Train: 0.0743 / 97.71% | Test: 0.0252 / 99.14% * | 8.4s\n",
            "E03 | Train: 0.0574 / 98.27% | Test: 0.0214 / 99.27% * | 8.2s\n",
            "E04 | Train: 0.0488 / 98.48% | Test: 0.0224 / 99.19% | 8.3s\n",
            "E05 | Train: 0.0416 / 98.72% | Test: 0.0177 / 99.37% * | 7.9s\n",
            "E06 | Train: 0.0349 / 98.88% | Test: 0.0186 / 99.37% | 8.2s\n",
            "E07 | Train: 0.0310 / 99.05% | Test: 0.0183 / 99.43% * | 8.4s\n",
            "E08 | Train: 0.0254 / 99.17% | Test: 0.0156 / 99.45% * | 8.1s\n",
            "E09 | Train: 0.0232 / 99.28% | Test: 0.0153 / 99.45% | 8.5s\n",
            "E10 | Train: 0.0209 / 99.33% | Test: 0.0149 / 99.47% * | 8.0s\n",
            "Best: 99.47% | Total: 82.6s\n",
            "\n",
            "============================================================\n",
            "Training: topo_spatial (p=0.2)\n",
            "============================================================\n",
            "E01 | Train: 0.1729 / 95.78% | Test: 0.0311 / 99.01% * | 8.2s\n",
            "E02 | Train: 0.0551 / 98.33% | Test: 0.0281 / 99.03% * | 7.9s\n",
            "E03 | Train: 0.0411 / 98.71% | Test: 0.0235 / 99.22% * | 8.5s\n",
            "E04 | Train: 0.0346 / 98.90% | Test: 0.0234 / 99.31% * | 8.1s\n",
            "E05 | Train: 0.0276 / 99.15% | Test: 0.0189 / 99.40% * | 8.4s\n",
            "E06 | Train: 0.0223 / 99.30% | Test: 0.0178 / 99.45% * | 8.7s\n",
            "E07 | Train: 0.0193 / 99.38% | Test: 0.0152 / 99.52% * | 8.2s\n",
            "E08 | Train: 0.0160 / 99.51% | Test: 0.0164 / 99.42% | 8.8s\n",
            "E09 | Train: 0.0140 / 99.55% | Test: 0.0147 / 99.50% | 8.2s\n",
            "E10 | Train: 0.0128 / 99.61% | Test: 0.0154 / 99.46% | 8.1s\n",
            "Best: 99.52% | Total: 83.3s\n",
            "\n",
            "======================================================================\n",
            "RESULTS SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Dropout Type         Best Acc     Final Acc    Time       Train-Test Gap \n",
            "----------------------------------------------------------------------\n",
            "topo_scheduled       99.55       % 99.55       % 83.3      s 0.23           %\n",
            "standard             99.52       % 99.49       % 81.6      s 0.20           %\n",
            "topo_spatial         99.52       % 99.46       % 83.3      s 0.15           %\n",
            "spatial              99.47       % 99.47       % 82.6      s -0.14          %\n",
            "topo                 99.42       % 99.42       % 82.0      s 0.35           %\n",
            "none                 99.40       % 99.39       % 83.3      s 0.61           %\n",
            "topo_importance      99.32       % 99.32       % 83.1      s 0.56           %\n",
            "\n",
            "--- vs Standard Dropout ---\n",
            "topo_scheduled       +0.03%\n",
            "topo_spatial         +0.00%\n",
            "spatial              -0.05%\n",
            "topo                 -0.10%\n",
            "none                 -0.12%\n",
            "topo_importance      -0.20%\n",
            "\n",
            "======================================================================\n",
            "GENERALIZATION ANALYSIS (Train-Test Gap Over Time)\n",
            "======================================================================\n",
            "\n",
            "Dropout Type         E1 Gap     E5 Gap     E10 Gap    Trend     \n",
            "------------------------------------------------------------\n",
            "topo_scheduled       -1.68     % -0.16     % 0.23      % ↑ overfit \n",
            "standard             -3.75     % -0.30     % 0.20      % ↑ overfit \n",
            "topo_spatial         -3.23     % -0.25     % 0.15      % ↑ overfit \n",
            "spatial              -4.42     % -0.65     % -0.14     % ↑ overfit \n",
            "topo                 -2.78     % -0.03     % 0.35      % ↑ overfit \n",
            "none                 -1.24     % 0.48      % 0.61      % ↑ overfit \n",
            "topo_importance      0.12      % 0.49      % 0.56      % → stable  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# dropouts fashionmnist"
      ],
      "metadata": {
        "id": "5fuWTCGw-C1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "\n",
        "# =============================================================================\n",
        "# TOPOLOGICAL DROPOUT VARIANTS\n",
        "# =============================================================================\n",
        "\n",
        "class TopologicalDropout(nn.Module):\n",
        "    \"\"\"\n",
        "    Structure-preserving dropout: drops entire routes/channels, not individual neurons.\n",
        "\n",
        "    For CNNs: treats channels as routes (drops entire feature maps)\n",
        "    For attention: treats heads/routes as units\n",
        "\n",
        "    Key insight: Preserves internal structure of surviving routes.\n",
        "    Standard dropout: Random holes everywhere → broken features\n",
        "    Topo dropout: Some features fully on, others fully off → intact features\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob: float = 0.1, min_keep: int = 1, route_dim: int = 1):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.min_keep = min_keep\n",
        "        self.route_dim = route_dim  # Which dim contains \"routes\" (channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if not self.training or self.drop_prob == 0:\n",
        "            return x\n",
        "\n",
        "        num_routes = x.shape[self.route_dim]\n",
        "        num_keep = max(self.min_keep, int(num_routes * (1 - self.drop_prob)))\n",
        "\n",
        "        # Random mask\n",
        "        mask = torch.zeros(num_routes, device=x.device)\n",
        "        perm = torch.randperm(num_routes, device=x.device)[:num_keep]\n",
        "        mask[perm] = 1.0\n",
        "\n",
        "        # Scale to preserve expected value\n",
        "        mask = mask * (num_routes / num_keep)\n",
        "\n",
        "        # Reshape for broadcast\n",
        "        shape = [1] * x.dim()\n",
        "        shape[self.route_dim] = num_routes\n",
        "\n",
        "        return x * mask.view(shape)\n",
        "\n",
        "\n",
        "class ImportanceTopologicalDropout(nn.Module):\n",
        "    \"\"\"\n",
        "    Topo dropout with importance weighting.\n",
        "    Less important routes more likely to be dropped.\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob: float = 0.1, min_keep: int = 1, route_dim: int = 1):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.min_keep = min_keep\n",
        "        self.route_dim = route_dim\n",
        "\n",
        "    def forward(self, x: torch.Tensor, importance: torch.Tensor = None) -> torch.Tensor:\n",
        "        if not self.training or self.drop_prob == 0:\n",
        "            return x\n",
        "\n",
        "        num_routes = x.shape[self.route_dim]\n",
        "        num_keep = max(self.min_keep, int(num_routes * (1 - self.drop_prob)))\n",
        "\n",
        "        if importance is None:\n",
        "            # Use activation magnitude as importance proxy\n",
        "            # Reduce all dims except route_dim\n",
        "            reduce_dims = [i for i in range(x.dim()) if i != self.route_dim]\n",
        "            importance = x.abs().mean(dim=reduce_dims)\n",
        "\n",
        "        # Add noise to importance for stochasticity\n",
        "        noise = torch.rand_like(importance) * 0.3\n",
        "        scores = importance + noise\n",
        "\n",
        "        # Keep top-k by importance\n",
        "        _, keep_idx = scores.topk(num_keep)\n",
        "        mask = torch.zeros(num_routes, device=x.device)\n",
        "        mask[keep_idx] = 1.0\n",
        "\n",
        "        # Scale\n",
        "        mask = mask * (num_routes / num_keep)\n",
        "\n",
        "        shape = [1] * x.dim()\n",
        "        shape[self.route_dim] = num_routes\n",
        "\n",
        "        return x * mask.view(shape)\n",
        "\n",
        "\n",
        "class ScheduledTopologicalDropout(nn.Module):\n",
        "    \"\"\"\n",
        "    Topo dropout with warmup schedule.\n",
        "    Starts mild, increases over training.\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob: float = 0.2, min_keep: int = 1, route_dim: int = 1,\n",
        "                 warmup_steps: int = 1000):\n",
        "        super().__init__()\n",
        "        self.target_drop_prob = drop_prob\n",
        "        self.min_keep = min_keep\n",
        "        self.route_dim = route_dim\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.register_buffer('step', torch.tensor(0))\n",
        "\n",
        "    @property\n",
        "    def current_drop_prob(self):\n",
        "        progress = min(1.0, self.step.item() / self.warmup_steps)\n",
        "        return self.target_drop_prob * progress\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.training:\n",
        "            self.step += 1\n",
        "\n",
        "        if not self.training or self.current_drop_prob == 0:\n",
        "            return x\n",
        "\n",
        "        num_routes = x.shape[self.route_dim]\n",
        "        num_keep = max(self.min_keep, int(num_routes * (1 - self.current_drop_prob)))\n",
        "\n",
        "        mask = torch.zeros(num_routes, device=x.device)\n",
        "        perm = torch.randperm(num_routes, device=x.device)[:num_keep]\n",
        "        mask[perm] = 1.0\n",
        "        mask = mask * (num_routes / num_keep)\n",
        "\n",
        "        shape = [1] * x.dim()\n",
        "        shape[self.route_dim] = num_routes\n",
        "\n",
        "        return x * mask.view(shape)\n",
        "\n",
        "\n",
        "class SpatialTopologicalDropout(nn.Module):\n",
        "    \"\"\"\n",
        "    For 2D feature maps: drops entire spatial regions (patches).\n",
        "    Complements channel-wise topo dropout.\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob: float = 0.1, patch_size: int = 2):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if not self.training or self.drop_prob == 0:\n",
        "            return x\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "        pH, pW = H // self.patch_size, W // self.patch_size\n",
        "\n",
        "        if pH == 0 or pW == 0:\n",
        "            return x\n",
        "\n",
        "        # Create patch mask\n",
        "        mask = (torch.rand(B, 1, pH, pW, device=x.device) > self.drop_prob).float()\n",
        "\n",
        "        # Scale surviving patches\n",
        "        keep_ratio = mask.mean()\n",
        "        if keep_ratio > 0:\n",
        "            mask = mask / keep_ratio\n",
        "\n",
        "        # Upsample mask to full resolution\n",
        "        mask = F.interpolate(mask, size=(H, W), mode='nearest')\n",
        "\n",
        "        return x * mask\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIG\n",
        "# =============================================================================\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "LR = 1e-3\n",
        "\n",
        "print(f\"Device: {DEVICE}\")\n",
        "\n",
        "# =============================================================================\n",
        "# DATA\n",
        "# =============================================================================\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.FashionMNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.FashionMNIST('./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train: {len(train_dataset):,} | Test: {len(test_dataset):,}\")\n",
        "\n",
        "# =============================================================================\n",
        "# MODEL\n",
        "# =============================================================================\n",
        "\n",
        "class MNISTClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN with configurable dropout strategy.\n",
        "\n",
        "    dropout_type options:\n",
        "    - 'standard': nn.Dropout (element-wise)\n",
        "    - 'topo': TopologicalDropout (channel-wise)\n",
        "    - 'topo_importance': ImportanceTopologicalDropout\n",
        "    - 'topo_scheduled': ScheduledTopologicalDropout\n",
        "    - 'spatial': SpatialTopologicalDropout\n",
        "    - 'topo_spatial': Both channel and spatial topo dropout\n",
        "    - 'none': No dropout\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout_type='topo', drop_prob=0.2):\n",
        "        super().__init__()\n",
        "        self.dropout_type = dropout_type\n",
        "\n",
        "        # Conv layers\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "\n",
        "        # FC layers\n",
        "        self.fc1 = nn.Linear(128 * 3 * 3, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "        # Batch norms\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
        "        self.bn_fc2 = nn.BatchNorm1d(128)\n",
        "\n",
        "        # Dropout variants\n",
        "        if dropout_type == 'standard':\n",
        "            self.drop_conv = nn.Dropout2d(drop_prob)\n",
        "            self.drop_fc = nn.Dropout(drop_prob)\n",
        "            self.spatial_drop = None\n",
        "        elif dropout_type == 'topo':\n",
        "            self.drop_conv = TopologicalDropout(drop_prob, min_keep=4, route_dim=1)\n",
        "            self.drop_fc = TopologicalDropout(drop_prob, min_keep=16, route_dim=1)\n",
        "            self.spatial_drop = None\n",
        "        elif dropout_type == 'topo_importance':\n",
        "            self.drop_conv = ImportanceTopologicalDropout(drop_prob, min_keep=4, route_dim=1)\n",
        "            self.drop_fc = ImportanceTopologicalDropout(drop_prob, min_keep=16, route_dim=1)\n",
        "            self.spatial_drop = None\n",
        "        elif dropout_type == 'topo_scheduled':\n",
        "            self.drop_conv = ScheduledTopologicalDropout(drop_prob, min_keep=4, route_dim=1, warmup_steps=500)\n",
        "            self.drop_fc = ScheduledTopologicalDropout(drop_prob, min_keep=16, route_dim=1, warmup_steps=500)\n",
        "            self.spatial_drop = None\n",
        "        elif dropout_type == 'spatial':\n",
        "            self.drop_conv = SpatialTopologicalDropout(drop_prob, patch_size=2)\n",
        "            self.drop_fc = nn.Dropout(drop_prob)\n",
        "            self.spatial_drop = None\n",
        "        elif dropout_type == 'topo_spatial':\n",
        "            self.drop_conv = TopologicalDropout(drop_prob * 0.5, min_keep=4, route_dim=1)\n",
        "            self.drop_fc = TopologicalDropout(drop_prob, min_keep=16, route_dim=1)\n",
        "            self.spatial_drop = SpatialTopologicalDropout(drop_prob * 0.5, patch_size=2)\n",
        "        elif dropout_type == 'none':\n",
        "            self.drop_conv = nn.Identity()\n",
        "            self.drop_fc = nn.Identity()\n",
        "            self.spatial_drop = None\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown dropout_type: {dropout_type}\")\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Conv block 1: 28x28 -> 14x14\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop_conv(x)\n",
        "        if self.spatial_drop:\n",
        "            x = self.spatial_drop(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Conv block 2: 14x14 -> 7x7\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop_conv(x)\n",
        "        if self.spatial_drop:\n",
        "            x = self.spatial_drop(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Conv block 3: 7x7 -> 3x3\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop_conv(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # FC layers\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn_fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop_fc(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn_fc2(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop_fc(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(model, loader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for data, target in loader:\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += pred.eq(target).sum().item()\n",
        "        total += target.size(0)\n",
        "\n",
        "    return total_loss / len(loader), 100. * correct / total\n",
        "\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            output = model(data)\n",
        "            total_loss += F.cross_entropy(output, target).item()\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "    return total_loss / len(loader), 100. * correct / total\n",
        "\n",
        "\n",
        "def train_model(dropout_type='topo', drop_prob=0.2, verbose=True):\n",
        "    model = MNISTClassifier(dropout_type=dropout_type, drop_prob=drop_prob).to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Training: {dropout_type} (p={drop_prob})\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "    best_acc = 0\n",
        "    history = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer)\n",
        "        test_loss, test_acc = evaluate(model, test_loader)\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        best_marker = \" *\" if test_acc > best_acc else \"\"\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"E{epoch:02d} | Train: {train_loss:.4f} / {train_acc:.2f}% | \"\n",
        "                  f\"Test: {test_loss:.4f} / {test_acc:.2f}%{best_marker} | {epoch_time:.1f}s\")\n",
        "\n",
        "        history.append({'epoch': epoch, 'train_loss': train_loss, 'train_acc': train_acc,\n",
        "                       'test_loss': test_loss, 'test_acc': test_acc, 'time': epoch_time})\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Best: {best_acc:.2f}% | Total: {total_time:.1f}s\")\n",
        "\n",
        "    return model, history, best_acc, total_time\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# RUN EXPERIMENTS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TOPOLOGICAL DROPOUT EXPERIMENT 2 - MNIST Fashion\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Test each dropout variant\n",
        "dropout_configs = [\n",
        "    ('none', 0.0),\n",
        "    ('standard', 0.2),\n",
        "    ('topo', 0.2),\n",
        "    ('topo_importance', 0.2),\n",
        "    ('topo_scheduled', 0.2),\n",
        "    ('spatial', 0.2),\n",
        "    ('topo_spatial', 0.2),\n",
        "]\n",
        "\n",
        "for dropout_type, drop_prob in dropout_configs:\n",
        "    model, history, best_acc, total_time = train_model(dropout_type, drop_prob)\n",
        "    results[dropout_type] = {\n",
        "        'best_acc': best_acc,\n",
        "        'final_acc': history[-1]['test_acc'],\n",
        "        'time': total_time,\n",
        "        'history': history\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n{'Dropout Type':<20} {'Best Acc':<12} {'Final Acc':<12} {'Time':<10} {'Train-Test Gap':<15}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Sort by best accuracy\n",
        "sorted_results = sorted(results.items(), key=lambda x: x[1]['best_acc'], reverse=True)\n",
        "\n",
        "for dropout_type, data in sorted_results:\n",
        "    final_train = data['history'][-1]['train_acc']\n",
        "    final_test = data['history'][-1]['test_acc']\n",
        "    gap = final_train - final_test\n",
        "    print(f\"{dropout_type:<20} {data['best_acc']:<12.2f}% {data['final_acc']:<12.2f}% \"\n",
        "          f\"{data['time']:<10.1f}s {gap:<15.2f}%\")\n",
        "\n",
        "# Best vs baseline\n",
        "baseline = results['standard']['best_acc']\n",
        "print(f\"\\n--- vs Standard Dropout ---\")\n",
        "for dropout_type, data in sorted_results:\n",
        "    if dropout_type != 'standard':\n",
        "        diff = data['best_acc'] - baseline\n",
        "        print(f\"{dropout_type:<20} {diff:+.2f}%\")\n",
        "\n",
        "# =============================================================================\n",
        "# GENERALIZATION ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"GENERALIZATION ANALYSIS (Train-Test Gap Over Time)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n{'Dropout Type':<20} {'E1 Gap':<10} {'E5 Gap':<10} {'E10 Gap':<10} {'Trend':<10}\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "for dropout_type, data in sorted_results:\n",
        "    h = data['history']\n",
        "    gap_1 = h[0]['train_acc'] - h[0]['test_acc']\n",
        "    gap_5 = h[4]['train_acc'] - h[4]['test_acc']\n",
        "    gap_10 = h[9]['train_acc'] - h[9]['test_acc']\n",
        "\n",
        "    if gap_10 < gap_1:\n",
        "        trend = \"↓ good\"\n",
        "    elif gap_10 > gap_1 + 1:\n",
        "        trend = \"↑ overfit\"\n",
        "    else:\n",
        "        trend = \"→ stable\"\n",
        "\n",
        "    print(f\"{dropout_type:<20} {gap_1:<10.2f}% {gap_5:<10.2f}% {gap_10:<10.2f}% {trend:<10}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bC4qVj5-Enx",
        "outputId": "1c0f82e0-1796-4ef1-8731-55fee9749a76"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 9.71MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 185kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.57MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 17.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 60,000 | Test: 10,000\n",
            "\n",
            "======================================================================\n",
            "TOPOLOGICAL DROPOUT EXPERIMENT - MNIST\n",
            "======================================================================\n",
            "\n",
            "============================================================\n",
            "Training: none (p=0.0)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E01 | Train: 0.3570 / 87.82% | Test: 0.2878 / 89.31% * | 8.2s\n",
            "E02 | Train: 0.2209 / 91.86% | Test: 0.2457 / 91.27% * | 8.4s\n",
            "E03 | Train: 0.1792 / 93.42% | Test: 0.2370 / 91.34% * | 8.6s\n",
            "E04 | Train: 0.1427 / 94.78% | Test: 0.2227 / 92.19% * | 8.6s\n",
            "E05 | Train: 0.1093 / 95.96% | Test: 0.2426 / 92.14% | 8.4s\n",
            "E06 | Train: 0.0787 / 97.15% | Test: 0.2372 / 92.68% * | 8.3s\n",
            "E07 | Train: 0.0484 / 98.33% | Test: 0.2590 / 92.75% * | 8.0s\n",
            "E08 | Train: 0.0272 / 99.15% | Test: 0.2797 / 92.46% | 8.0s\n",
            "E09 | Train: 0.0141 / 99.65% | Test: 0.2785 / 93.01% * | 8.5s\n",
            "E10 | Train: 0.0090 / 99.87% | Test: 0.2821 / 93.06% * | 8.1s\n",
            "Best: 93.06% | Total: 83.1s\n",
            "\n",
            "============================================================\n",
            "Training: standard (p=0.2)\n",
            "============================================================\n",
            "E01 | Train: 0.4817 / 83.29% | Test: 0.3101 / 88.74% * | 8.7s\n",
            "E02 | Train: 0.3107 / 88.54% | Test: 0.2800 / 89.34% * | 8.2s\n",
            "E03 | Train: 0.2680 / 90.15% | Test: 0.2490 / 90.74% * | 8.7s\n",
            "E04 | Train: 0.2445 / 90.94% | Test: 0.2437 / 91.02% * | 8.0s\n",
            "E05 | Train: 0.2231 / 91.78% | Test: 0.2218 / 91.81% * | 8.2s\n",
            "E06 | Train: 0.2060 / 92.39% | Test: 0.2219 / 92.00% * | 8.3s\n",
            "E07 | Train: 0.1894 / 92.94% | Test: 0.2085 / 92.33% * | 8.6s\n",
            "E08 | Train: 0.1762 / 93.48% | Test: 0.2138 / 92.30% | 8.0s\n",
            "E09 | Train: 0.1675 / 93.76% | Test: 0.2041 / 92.82% * | 8.2s\n",
            "E10 | Train: 0.1621 / 94.00% | Test: 0.2035 / 92.77% | 8.1s\n",
            "Best: 92.82% | Total: 82.9s\n",
            "\n",
            "============================================================\n",
            "Training: topo (p=0.2)\n",
            "============================================================\n",
            "E01 | Train: 0.4486 / 84.56% | Test: 0.3536 / 87.00% * | 8.5s\n",
            "E02 | Train: 0.2923 / 89.21% | Test: 0.2591 / 90.38% * | 8.5s\n",
            "E03 | Train: 0.2545 / 90.70% | Test: 0.2564 / 90.56% * | 8.4s\n",
            "E04 | Train: 0.2292 / 91.62% | Test: 0.2377 / 91.11% * | 8.5s\n",
            "E05 | Train: 0.2111 / 92.17% | Test: 0.2338 / 91.23% * | 8.4s\n",
            "E06 | Train: 0.1961 / 92.84% | Test: 0.2319 / 91.37% * | 8.1s\n",
            "E07 | Train: 0.1779 / 93.42% | Test: 0.2074 / 92.64% * | 8.6s\n",
            "E08 | Train: 0.1658 / 93.81% | Test: 0.2135 / 92.29% | 8.4s\n",
            "E09 | Train: 0.1544 / 94.27% | Test: 0.2034 / 92.88% * | 8.5s\n",
            "E10 | Train: 0.1493 / 94.43% | Test: 0.2077 / 92.42% | 8.3s\n",
            "Best: 92.88% | Total: 84.1s\n",
            "\n",
            "============================================================\n",
            "Training: topo_importance (p=0.2)\n",
            "============================================================\n",
            "E01 | Train: 0.4513 / 84.38% | Test: 0.3609 / 86.84% * | 8.2s\n",
            "E02 | Train: 0.2881 / 89.34% | Test: 0.3198 / 88.86% * | 8.5s\n",
            "E03 | Train: 0.2498 / 90.77% | Test: 0.3062 / 89.05% * | 8.3s\n",
            "E04 | Train: 0.2199 / 91.96% | Test: 0.2802 / 89.93% * | 8.2s\n",
            "E05 | Train: 0.1993 / 92.59% | Test: 0.2665 / 90.37% * | 7.9s\n",
            "E06 | Train: 0.1773 / 93.35% | Test: 0.2736 / 90.13% | 8.7s\n",
            "E07 | Train: 0.1558 / 94.21% | Test: 0.2974 / 89.75% | 8.1s\n",
            "E08 | Train: 0.1389 / 94.79% | Test: 0.2708 / 90.33% | 7.9s\n",
            "E09 | Train: 0.1258 / 95.30% | Test: 0.2792 / 90.26% | 8.2s\n",
            "E10 | Train: 0.1181 / 95.66% | Test: 0.2700 / 90.48% * | 8.4s\n",
            "Best: 90.48% | Total: 82.4s\n",
            "\n",
            "============================================================\n",
            "Training: topo_scheduled (p=0.2)\n",
            "============================================================\n",
            "E01 | Train: 0.4019 / 86.01% | Test: 0.3231 / 87.87% * | 8.2s\n",
            "E02 | Train: 0.2881 / 89.51% | Test: 0.2653 / 90.03% * | 8.1s\n",
            "E03 | Train: 0.2520 / 90.75% | Test: 0.3116 / 88.68% | 8.2s\n",
            "E04 | Train: 0.2310 / 91.30% | Test: 0.2395 / 91.21% * | 8.1s\n",
            "E05 | Train: 0.2089 / 92.29% | Test: 0.2282 / 91.86% * | 8.2s\n",
            "E06 | Train: 0.1902 / 93.10% | Test: 0.2174 / 92.32% * | 8.3s\n",
            "E07 | Train: 0.1740 / 93.58% | Test: 0.2115 / 92.58% * | 8.6s\n",
            "E08 | Train: 0.1615 / 94.03% | Test: 0.2069 / 92.77% * | 8.6s\n",
            "E09 | Train: 0.1515 / 94.48% | Test: 0.2014 / 93.01% * | 8.0s\n",
            "E10 | Train: 0.1453 / 94.62% | Test: 0.2033 / 92.96% | 8.3s\n",
            "Best: 93.01% | Total: 82.7s\n",
            "\n",
            "============================================================\n",
            "Training: spatial (p=0.2)\n",
            "============================================================\n",
            "E01 | Train: 0.4977 / 82.54% | Test: 0.3259 / 87.59% * | 8.1s\n",
            "E02 | Train: 0.3370 / 87.63% | Test: 0.2892 / 89.24% * | 8.2s\n",
            "E03 | Train: 0.2918 / 89.29% | Test: 0.2698 / 89.81% * | 8.2s\n",
            "E04 | Train: 0.2700 / 90.16% | Test: 0.2550 / 90.41% * | 8.2s\n",
            "E05 | Train: 0.2494 / 90.78% | Test: 0.2227 / 91.81% * | 8.1s\n",
            "E06 | Train: 0.2294 / 91.51% | Test: 0.2210 / 91.80% | 8.5s\n",
            "E07 | Train: 0.2124 / 92.09% | Test: 0.2115 / 92.12% * | 8.4s\n",
            "E08 | Train: 0.2001 / 92.66% | Test: 0.2022 / 92.60% * | 8.4s\n",
            "E09 | Train: 0.1910 / 92.98% | Test: 0.2000 / 92.78% * | 8.3s\n",
            "E10 | Train: 0.1836 / 93.26% | Test: 0.1976 / 92.64% | 8.1s\n",
            "Best: 92.78% | Total: 82.6s\n",
            "\n",
            "============================================================\n",
            "Training: topo_spatial (p=0.2)\n",
            "============================================================\n",
            "E01 | Train: 0.4674 / 83.97% | Test: 0.2972 / 89.15% * | 8.0s\n",
            "E02 | Train: 0.3044 / 88.90% | Test: 0.2781 / 89.62% * | 8.0s\n",
            "E03 | Train: 0.2660 / 90.27% | Test: 0.2539 / 90.44% * | 8.2s\n",
            "E04 | Train: 0.2435 / 91.03% | Test: 0.2396 / 91.12% * | 8.3s\n",
            "E05 | Train: 0.2251 / 91.72% | Test: 0.2412 / 91.07% | 8.1s\n",
            "E06 | Train: 0.2082 / 92.39% | Test: 0.2168 / 91.88% * | 8.1s\n",
            "E07 | Train: 0.1910 / 92.98% | Test: 0.2155 / 92.09% * | 8.2s\n",
            "E08 | Train: 0.1764 / 93.37% | Test: 0.2189 / 92.12% * | 8.3s\n",
            "E09 | Train: 0.1669 / 93.88% | Test: 0.2073 / 92.48% * | 8.1s\n",
            "E10 | Train: 0.1614 / 93.93% | Test: 0.2033 / 92.63% * | 8.4s\n",
            "Best: 92.63% | Total: 81.7s\n",
            "\n",
            "======================================================================\n",
            "RESULTS SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Dropout Type         Best Acc     Final Acc    Time       Train-Test Gap \n",
            "----------------------------------------------------------------------\n",
            "none                 93.06       % 93.06       % 83.1      s 6.81           %\n",
            "topo_scheduled       93.01       % 92.96       % 82.7      s 1.66           %\n",
            "topo                 92.88       % 92.42       % 84.1      s 2.01           %\n",
            "standard             92.82       % 92.77       % 82.9      s 1.23           %\n",
            "spatial              92.78       % 92.64       % 82.6      s 0.62           %\n",
            "topo_spatial         92.63       % 92.63       % 81.7      s 1.30           %\n",
            "topo_importance      90.48       % 90.48       % 82.4      s 5.17           %\n",
            "\n",
            "--- vs Standard Dropout ---\n",
            "none                 +0.24%\n",
            "topo_scheduled       +0.19%\n",
            "topo                 +0.06%\n",
            "spatial              -0.04%\n",
            "topo_spatial         -0.19%\n",
            "topo_importance      -2.34%\n",
            "\n",
            "======================================================================\n",
            "GENERALIZATION ANALYSIS (Train-Test Gap Over Time)\n",
            "======================================================================\n",
            "\n",
            "Dropout Type         E1 Gap     E5 Gap     E10 Gap    Trend     \n",
            "------------------------------------------------------------\n",
            "none                 -1.49     % 3.82      % 6.81      % ↑ overfit \n",
            "topo_scheduled       -1.86     % 0.43      % 1.66      % ↑ overfit \n",
            "topo                 -2.44     % 0.94      % 2.01      % ↑ overfit \n",
            "standard             -5.45     % -0.03     % 1.23      % ↑ overfit \n",
            "spatial              -5.05     % -1.03     % 0.62      % ↑ overfit \n",
            "topo_spatial         -5.18     % 0.65      % 1.30      % ↑ overfit \n",
            "topo_importance      -2.46     % 2.22      % 5.17      % ↑ overfit \n"
          ]
        }
      ]
    }
  ]
}